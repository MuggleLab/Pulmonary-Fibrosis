{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT\n",
    "\n",
    "this kernel is to preprocess DICOM images and 3D plot with ploty from the competition [OSIC Pulmonary Fibrosis Progression](https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression).\n",
    "\n",
    "DICON images are cropped, resized, segmentated, converted into Poly3DCollection and then saved into npy files for later use in deep learning.\n",
    "\n",
    "I will keep working on it and improve preprocessings\n",
    "\n",
    "Note that it is highly based on [OSIC |quick EDA + 3D plot with plotly](https://www.kaggle.com/sunpnwt12/osic-quick-eda-3d-plot-with-plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge gdcm -y # run this code for the first time\n",
    "import os\n",
    "# import gdcm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import figure_factory as FF\n",
    "\n",
    "import scipy.ndimage\n",
    "from skimage import measure, morphology\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "import random\n",
    "import pydicom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load train and test image\n",
    "\n",
    "DICOM_DIR = '../dataset/train'\n",
    "# DICOM_DIR_TEST = '../data/test'\n",
    "\n",
    "dicom_dict = defaultdict(list)\n",
    "dicom_dict_test = defaultdict(list)\n",
    "\n",
    "default_image_size = 512\n",
    "\n",
    "for dirname in os.listdir(DICOM_DIR):\n",
    "    path = os.path.join(DICOM_DIR, dirname)\n",
    "    dicom_dict[dirname].append(path)\n",
    "    \n",
    "# for dirname in os.listdir(DICOM_DIR_TEST):\n",
    "#     path = os.path.join(DICOM_DIR_TEST, dirname)\n",
    "#     dicom_dict_test[dirname].append(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### load_scan:\n",
    "# since there are a couple of dicom files that don't have 'ImagePositionPatient' attribute so instead,\n",
    "# i will use 'InstanceNumber' attribute for those\n",
    "\n",
    "# ### dicom_file:\n",
    "# 1. take index number of patient which stored in dict_dicom earlier\n",
    "# 2. this might be useful when you need to pick some random patient\n",
    "# 3. It also takes specific patient Id in case you need.\n",
    "# 4. Note that this function is going to read all file in taken path.\n",
    "\n",
    "# ### get_pixels_hu\n",
    "# 1. take dicom file which had called through dicom_file function\n",
    "# 2. It stacks up all the load slices of certain patient\n",
    "# 3. stacked slices will be calculated into Hounsfield Units\n",
    "\n",
    "\n",
    "\n",
    "def load_scan(path):\n",
    "    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n",
    "    length = len(slices)\n",
    "    actualLength = sum([1 if hasattr(x, 'ImagePositionPatient') else 0 for x in slices])\n",
    "    if length == actualLength:\n",
    "        slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "    else:\n",
    "        slices.sort(key = lambda x: float(x.InstanceNumber))\n",
    "        \n",
    "    return slices\n",
    "\n",
    "def get_pixels_hu(slices):\n",
    "    image = np.stack([s.pixel_array for s in slices])\n",
    "    # Convert to int16 (from sometimes int16), \n",
    "    # should be possible as values should always be low enough (<32k)\n",
    "    image = image.astype(np.int16)\n",
    "    # Set outside-of-scan pixels to 0\n",
    "    # The intercept is usually -1024, so air is approximately 0\n",
    "    image[image < -2000] = 0\n",
    "    \n",
    "    # Convert to Hounsfield units (HU)\n",
    "    for slice_number in range(len(slices)):\n",
    "        \n",
    "        intercept = slices[slice_number].RescaleIntercept\n",
    "        slope = slices[slice_number].RescaleSlope\n",
    "        \n",
    "        if slope != 1:\n",
    "            image[slice_number] = slope * image[slice_number].astype(np.float64)\n",
    "            image[slbice_number] = image[slice_number].astype(np.int16)\n",
    "            \n",
    "        image[slice_number] += np.int16(intercept)\n",
    "    \n",
    "    return np.array(image, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "\n",
    "* https://www.raddq.com/dicom-processing-segmentation-visualization-in-python/\n",
    "* https://medium.com/@hengloose/a-comprehensive-starter-guide-to-visualizing-and-analyzing-dicom-images-in-python-7a8430fcb7ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test = load_scan(dicom_dict['ID00014637202177757139317'][0])\n",
    "\n",
    "# test_hu = get_pixels_hu(test)\n",
    "# # print('Patient {}'.format(test[0].PatientName))\n",
    "# # print('Slices : {}\\nPixels : ({} x {})'.format(test_hu.shape[0], test_hu.shape[1], test_hu.shape[2]))\n",
    "\n",
    "# # plt.figure(figsize=(12, 8))\n",
    "# # ax = sns.distplot(test_hu.flatten(), bins=80, norm_hist=True)\n",
    "# # # ax.set_title('Hounsfield Units of patient {}'.format(test_hu[0].PatientName), fontsize=25)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref) https://stackoverflow.com/questions/48121916/numpy-resize-rescale-image\n",
    "# INTER_NEAREST - a nearest-neighbor interpolation\n",
    "# INTER_LINEAR - a bilinear interpolation (used by default)\n",
    "# INTER_AREA - resampling using pixel area relation. It may be a preferred method for image decimation, as it gives moireâ€™-free results. But when the image is zoomed, it is similar to the INTER_NEAREST method.\n",
    "# INTER_CUBIC - a bicubic interpolation over 4x4 pixel neighborhood\n",
    "# INTER_LANCZOS4 - a Lanczos interpolation over 8x8 pixel neighborhood\n",
    "\n",
    "import cv2\n",
    "def resize(slices):\n",
    "    new_slice = []\n",
    "    for slice_number in range(len(slices)):\n",
    "        new_slice.append(cv2.resize(slices[slice_number], dsize=(default_image_size, default_image_size), interpolation=cv2.INTER_AREA))\n",
    "    return np.array(new_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do crop here\n",
    "def crop(slices):\n",
    "    slice_height = len(slices[0])\n",
    "    slice_width = len(slices[0][0])\n",
    "    diff_height_half = int((slice_height - default_image_size) / 2)\n",
    "    diff_width_half = int((slice_width - default_image_size) / 2)\n",
    "    new_slice = []\n",
    "    for slice_number in range(len(slices)):\n",
    "        new_slice.append(slices[slice_number][diff_width_half: slice_width - diff_width_half, diff_height_half: slice_height - diff_height_half])\n",
    "    return np.array(new_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resample(image, slice_tickness, x_pixel_spacing, y_pixel_spacing, new_spacing=[1,1,1]):\n",
    "    # Determine current pixel spacing\n",
    "    spacing = np.array([slice_tickness, x_pixel_spacing, y_pixel_spacing], dtype=np.float32)\n",
    "\n",
    "    resize_factor = spacing / new_spacing\n",
    "    new_real_shape = image.shape * resize_factor\n",
    "    new_shape = np.round(new_real_shape)\n",
    "    real_resize_factor = new_shape / image.shape\n",
    "    new_spacing = spacing / real_resize_factor\n",
    "    \n",
    "    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor, mode='nearest')\n",
    "    \n",
    "    return image, new_spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_mesh(image, threshold):\n",
    "    p = image.transpose(2, 1, 0)\n",
    "    \n",
    "    verts, faces, normals, values = measure.marching_cubes_lewiner(p, threshold)\n",
    "    return verts, faces\n",
    "\n",
    "def static_3d(image, threshold=-300):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    verts, faces = make_mesh(image, threshold)\n",
    "    x, y, z = zip(*verts)\n",
    "    \n",
    "    mesh = Poly3DCollection(verts[faces], alpha=0.1)\n",
    "    face_color = [0.5, 0.5, 1]\n",
    "    mesh.set_facecolor(face_color)\n",
    "    \n",
    "    ax.add_collection3d(mesh)\n",
    "    ax.set_xlim(0, max(x))\n",
    "    ax.set_ylim(0, max(y))\n",
    "    ax.set_zlim(0, max(z))\n",
    "    plt.show()\n",
    "    \n",
    "def interactive_3d(image, threshold=-300):\n",
    "    verts, faces = make_mesh(image, threshold)\n",
    "    x, y, z = zip(*verts)\n",
    "    fig = FF.create_trisurf(x=x,\n",
    "                            y=y,\n",
    "                            z=z,\n",
    "                            plot_edges=False,\n",
    "                            simplices=faces)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_size = test[0].Columns\n",
    "# y_size = test[0].Rows\n",
    "# slice_thinkness = test[0].SliceThickness\n",
    "# x_pixel_spacing = test[0].PixelSpacing[0] * (x_size / default_image_size)\n",
    "# y_pixel_spacing = test[0].PixelSpacing[1] * (y_size / default_image_size)\n",
    "# # x_pixel_spacing = test[0].PixelSpacing[0]\n",
    "# # y_pixel_spacing = test[0].PixelSpacing[1]\n",
    "# print(test_hu.shape)\n",
    "# print(x_pixel_spacing, y_pixel_spacing)\n",
    "\n",
    "# # if ratio is not 1:1 then crop\n",
    "# if (x_size != y_size):\n",
    "#     test_hu = crop(test_hu)\n",
    "#     # if cropped, reset pixel spacing with cropped image size because images with margin merely have space\n",
    "#     # outter size of image so pixel spacing size shouldn't be affected by those space\n",
    "#     x_pixel_spacing = test[0].PixelSpacing[0] * (test_hu.shape[2] / default_image_size)\n",
    "#     y_pixel_spacing = test[0].PixelSpacing[1] * (test_hu.shape[1] / default_image_size)\n",
    "    \n",
    "# # if size is not 512 then resize\n",
    "# if (x_size != default_image_size or y_size != default_image_size):\n",
    "#     test_hu = resize(test_hu)\n",
    "\n",
    "# # resample test\n",
    "# resampled_test_hu, spacing = resample(test_hu, slice_thinkness, x_pixel_spacing, y_pixel_spacing)\n",
    "# print(resampled_test_hu.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.ndimage import zoom\n",
    "# z_med_size = 135\n",
    "# y_med_size = 180\n",
    "# x_med_size = 180\n",
    "# shape = resampled_test_hu.shape\n",
    "# print(z_med_size/shape[0], y_med_size/shape[1])\n",
    "# resampled_test_hu = zoom(resampled_test_hu, (z_med_size/shape[0], y_med_size/shape[1], x_med_size/shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of images that are not 1:1 are the ones with margins outside of images. crop those margins and resize to 512 resolution\n",
    "\n",
    "As total resolution gets smaller, I muliplied pixel spacings by the ration too ( I guess as total number of pixel gets smaller, single pixel covers larger space thus bigger millimetres ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampled_test_hu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_3d(resampled_test_hu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def largest_label_volume(im, bg=-1):\n",
    "    vals, counts = np.unique(im, return_counts=True)\n",
    "\n",
    "    counts = counts[vals != bg]\n",
    "    vals = vals[vals != bg]\n",
    "\n",
    "    if len(counts) > 0:\n",
    "        return vals[np.argmax(counts)]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def segment_lung_mask(image, fill_lung_structures=True):\n",
    "    # not actually binary, but 1 and 2. \n",
    "    # 0 is treated as background, which we do not want\n",
    "    binary_image = np.array(image > -320, dtype=np.int8)+1\n",
    "    labels = measure.label(binary_image)\n",
    "    \n",
    "    # Pick the pixel in the very corner to determine which label is air.\n",
    "    #   Improvement: Pick multiple background labels from around the patient\n",
    "    #   More resistant to \"trays\" on which the patient lays cutting the air \n",
    "    #   around the person in half\n",
    "    z, y, x = labels.shape\n",
    "    for slice_number in range(len(image)):\n",
    "        #Fill the air around the person\n",
    "        binary_image[slice_number][labels[slice_number, 0, 0] == labels[slice_number]] = 2\n",
    "        binary_image[slice_number][labels[slice_number, 0, x - 1] == labels[slice_number]] = 2\n",
    "        binary_image[slice_number][labels[slice_number, y - 1, 0] == labels[slice_number]] = 2\n",
    "        binary_image[slice_number][labels[slice_number, y - 1, x - 1] == labels[slice_number]] = 2\n",
    "        binary_image[slice_number][labels[slice_number, 0, int((x - 1) / 2)] == labels[slice_number]] = 2\n",
    "        binary_image[slice_number][labels[slice_number, y - 1, int((x - 1) / 2)] == labels[slice_number]] = 2\n",
    "    \n",
    "    # Method of filling the lung structures (that is superior to something like \n",
    "    # morphological closing)\n",
    "    if fill_lung_structures:\n",
    "        # For every slice we determine the largest solid structure\n",
    "        for i, axial_slice in enumerate(binary_image):\n",
    "            axial_slice = axial_slice - 1\n",
    "            labeling = measure.label(axial_slice)\n",
    "            l_max = largest_label_volume(labeling, bg=0)\n",
    "            \n",
    "            if l_max is not None: #This slice contains some lung\n",
    "                binary_image[i][labeling != l_max] = 1\n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%â‚©\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# segmented_lungs = segment_lung_mask(resampled_test_hu, False)\n",
    "# segmented_lungs_fill = segment_lung_mask(resampled_test_hu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmented lungs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_3d(segmented_lungs, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmented lungs filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_3d(segmented_lungs_fill, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_3d(segmented_lungs_fill - segmented_lungs, -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive_3d(segmented_lungs_fill - segmented_lungs, -0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Save to npy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the codes below is to save segmented lung data into npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directories\n",
    "# os.removedirs(\"/kaggle/working/train\")\n",
    "# os.removedirs(\"/kaggle/working/test\")\n",
    "# os.makedirs('train')\n",
    "# os.makedirs('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID00296637202279895784347 ['../dataset/train/ID00296637202279895784347']\n",
      "before resize (31, 366, 366)\n",
      "after resize (135, 180, 180)\n",
      "ID00032637202181710233084 ['../dataset/train/ID00032637202181710233084']\n",
      "before resize (256, 433, 433)\n",
      "after resize (135, 180, 180)\n",
      "ID00061637202188184085559 ['../dataset/train/ID00061637202188184085559']\n",
      "before resize (632, 389, 389)\n",
      "after resize (135, 180, 180)\n",
      "ID00232637202260377586117 ['../dataset/train/ID00232637202260377586117']\n",
      "before resize (335, 350, 350)\n",
      "after resize (135, 180, 180)\n",
      "ID00322637202284842245491 ['../dataset/train/ID00322637202284842245491']\n",
      "before resize (265, 354, 354)\n",
      "after resize (135, 180, 180)\n",
      "ID00329637202285906759848 ['../dataset/train/ID00329637202285906759848']\n",
      "before resize (260, 385, 385)\n",
      "after resize (135, 180, 180)\n",
      "ID00027637202179689871102 ['../dataset/train/ID00027637202179689871102']\n",
      "before resize (716, 454, 454)\n",
      "after resize (135, 180, 180)\n",
      "ID00011637202177653955184 ['../dataset/train/ID00011637202177653955184']\n",
      "ID00042637202184406822975 ['../dataset/train/ID00042637202184406822975']\n",
      "before resize (621, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00086637202203494931510 ['../dataset/train/ID00086637202203494931510']\n",
      "before resize (38, 343, 343)\n",
      "after resize (135, 180, 180)\n",
      "ID00023637202179104603099 ['../dataset/train/ID00023637202179104603099']\n",
      "before resize (27, 290, 290)\n",
      "after resize (135, 180, 180)\n",
      "ID00235637202261451839085 ['../dataset/train/ID00235637202261451839085']\n",
      "before resize (296, 386, 386)\n",
      "after resize (135, 180, 180)\n",
      "ID00168637202237852027833 ['../dataset/train/ID00168637202237852027833']\n",
      "before resize (759, 375, 375)\n",
      "after resize (135, 180, 180)\n",
      "ID00078637202199415319443 ['../dataset/train/ID00078637202199415319443']\n",
      "before resize (636, 500, 500)\n",
      "after resize (135, 180, 180)\n",
      "ID00331637202286306023714 ['../dataset/train/ID00331637202286306023714']\n",
      "before resize (370, 380, 380)\n",
      "after resize (135, 180, 180)\n",
      "ID00312637202282607344793 ['../dataset/train/ID00312637202282607344793']\n",
      "before resize (256, 340, 340)\n",
      "after resize (135, 180, 180)\n",
      "ID00283637202278714365037 ['../dataset/train/ID00283637202278714365037']\n",
      "before resize (612, 430, 430)\n",
      "after resize (135, 180, 180)\n",
      "ID00365637202296085035729 ['../dataset/train/ID00365637202296085035729']\n",
      "before resize (330, 309, 309)\n",
      "after resize (135, 180, 180)\n",
      "ID00414637202310318891556 ['../dataset/train/ID00414637202310318891556']\n",
      "before resize (250, 350, 350)\n",
      "after resize (135, 180, 180)\n",
      "ID00267637202270790561585 ['../dataset/train/ID00267637202270790561585']\n",
      "before resize (315, 320, 320)\n",
      "after resize (135, 180, 180)\n",
      "ID00400637202305055099402 ['../dataset/train/ID00400637202305055099402']\n",
      "before resize (331, 367, 367)\n",
      "after resize (135, 180, 180)\n",
      "ID00076637202199015035026 ['../dataset/train/ID00076637202199015035026']\n",
      "before resize (320, 400, 400)\n",
      "after resize (135, 180, 180)\n",
      "ID00173637202238329754031 ['../dataset/train/ID00173637202238329754031']\n",
      "before resize (752, 380, 380)\n",
      "after resize (135, 180, 180)\n",
      "ID00192637202245493238298 ['../dataset/train/ID00192637202245493238298']\n",
      "before resize (245, 420, 420)\n",
      "after resize (135, 180, 180)\n",
      "ID00364637202296074419422 ['../dataset/train/ID00364637202296074419422']\n",
      "before resize (33, 365, 365)\n",
      "after resize (135, 180, 180)\n",
      "ID00180637202240177410333 ['../dataset/train/ID00180637202240177410333']\n",
      "before resize (288, 359, 359)\n",
      "after resize (135, 180, 180)\n",
      "ID00132637202222178761324 ['../dataset/train/ID00132637202222178761324']\n",
      "before resize (407, 351, 351)\n",
      "after resize (135, 180, 180)\n",
      "ID00048637202185016727717 ['../dataset/train/ID00048637202185016727717']\n",
      "before resize (32, 384, 384)\n",
      "after resize (135, 180, 180)\n",
      "ID00407637202308788732304 ['../dataset/train/ID00407637202308788732304']\n",
      "before resize (54, 400, 400)\n",
      "after resize (135, 180, 180)\n",
      "ID00426637202313170790466 ['../dataset/train/ID00426637202313170790466']\n",
      "before resize (408, 368, 368)\n",
      "after resize (135, 180, 180)\n",
      "ID00167637202237397919352 ['../dataset/train/ID00167637202237397919352']\n",
      "before resize (276, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00136637202224951350618 ['../dataset/train/ID00136637202224951350618']\n",
      "before resize (303, 380, 380)\n",
      "after resize (135, 180, 180)\n",
      "ID00305637202281772703145 ['../dataset/train/ID00305637202281772703145']\n",
      "before resize (305, 333, 333)\n",
      "after resize (135, 180, 180)\n",
      "ID00291637202279398396106 ['../dataset/train/ID00291637202279398396106']\n",
      "before resize (485, 412, 412)\n",
      "after resize (135, 180, 180)\n",
      "ID00298637202280361773446 ['../dataset/train/ID00298637202280361773446']\n",
      "before resize (33, 320, 320)\n",
      "after resize (135, 180, 180)\n",
      "ID00393637202302431697467 ['../dataset/train/ID00393637202302431697467']\n",
      "before resize (478, 377, 377)\n",
      "after resize (135, 180, 180)\n",
      "ID00051637202185848464638 ['../dataset/train/ID00051637202185848464638']\n",
      "before resize (244, 334, 334)\n",
      "after resize (135, 180, 180)\n",
      "ID00129637202219868188000 ['../dataset/train/ID00129637202219868188000']\n",
      "before resize (27, 357, 357)\n",
      "after resize (135, 180, 180)\n",
      "ID00184637202242062969203 ['../dataset/train/ID00184637202242062969203']\n",
      "before resize (310, 370, 370)\n",
      "after resize (135, 180, 180)\n",
      "ID00068637202190879923934 ['../dataset/train/ID00068637202190879923934']\n",
      "before resize (354, 400, 400)\n",
      "after resize (135, 180, 180)\n",
      "ID00014637202177757139317 ['../dataset/train/ID00014637202177757139317']\n",
      "before resize (39, 410, 409)\n",
      "after resize (135, 180, 180)\n",
      "ID00093637202205278167493 ['../dataset/train/ID00093637202205278167493']\n",
      "before resize (37, 451, 451)\n",
      "after resize (135, 180, 180)\n",
      "ID00376637202297677828573 ['../dataset/train/ID00376637202297677828573']\n",
      "before resize (495, 426, 426)\n",
      "after resize (135, 180, 180)\n",
      "ID00047637202184938901501 ['../dataset/train/ID00047637202184938901501']\n",
      "before resize (206, 355, 355)\n",
      "after resize (135, 180, 180)\n",
      "ID00052637202186188008618 ['../dataset/train/ID00052637202186188008618']\n",
      "ID00117637202212360228007 ['../dataset/train/ID00117637202212360228007']\n",
      "before resize (30, 351, 351)\n",
      "after resize (135, 180, 180)\n",
      "ID00317637202283194142136 ['../dataset/train/ID00317637202283194142136']\n",
      "before resize (326, 347, 347)\n",
      "after resize (135, 180, 180)\n",
      "ID00299637202280383305867 ['../dataset/train/ID00299637202280383305867']\n",
      "before resize (310, 358, 358)\n",
      "after resize (135, 180, 180)\n",
      "ID00104637202208063407045 ['../dataset/train/ID00104637202208063407045']\n",
      "before resize (249, 340, 340)\n",
      "after resize (135, 180, 180)\n",
      "ID00138637202231603868088 ['../dataset/train/ID00138637202231603868088']\n",
      "before resize (304, 394, 394)\n",
      "after resize (135, 180, 180)\n",
      "ID00122637202216437668965 ['../dataset/train/ID00122637202216437668965']\n",
      "before resize (44, 411, 411)\n",
      "after resize (135, 180, 180)\n",
      "ID00417637202310901214011 ['../dataset/train/ID00417637202310901214011']\n",
      "before resize (280, 386, 386)\n",
      "after resize (135, 180, 180)\n",
      "ID00111637202210956877205 ['../dataset/train/ID00111637202210956877205']\n",
      "before resize (302, 369, 369)\n",
      "after resize (135, 180, 180)\n",
      "ID00337637202286839091062 ['../dataset/train/ID00337637202286839091062']\n",
      "before resize (320, 408, 408)\n",
      "after resize (135, 180, 180)\n",
      "ID00367637202296290303449 ['../dataset/train/ID00367637202296290303449']\n",
      "before resize (266, 409, 409)\n",
      "after resize (135, 180, 180)\n",
      "ID00139637202231703564336 ['../dataset/train/ID00139637202231703564336']\n",
      "before resize (320, 340, 340)\n",
      "after resize (135, 180, 180)\n",
      "ID00186637202242472088675 ['../dataset/train/ID00186637202242472088675']\n",
      "before resize (306, 335, 335)\n",
      "after resize (135, 180, 180)\n",
      "ID00202637202249376026949 ['../dataset/train/ID00202637202249376026949']\n",
      "before resize (412, 340, 340)\n",
      "after resize (135, 180, 180)\n",
      "ID00199637202248141386743 ['../dataset/train/ID00199637202248141386743']\n",
      "before resize (435, 342, 342)\n",
      "after resize (135, 180, 180)\n",
      "ID00172637202238316925179 ['../dataset/train/ID00172637202238316925179']\n",
      "before resize (31, 390, 390)\n",
      "after resize (135, 180, 180)\n",
      "ID00219637202258203123958 ['../dataset/train/ID00219637202258203123958']\n",
      "before resize (375, 408, 408)\n",
      "after resize (135, 180, 180)\n",
      "ID00228637202259965313869 ['../dataset/train/ID00228637202259965313869']\n",
      "before resize (347, 295, 295)\n",
      "after resize (135, 180, 180)\n",
      "ID00161637202235731948764 ['../dataset/train/ID00161637202235731948764']\n",
      "before resize (28, 352, 352)\n",
      "after resize (135, 180, 180)\n",
      "ID00344637202287684217717 ['../dataset/train/ID00344637202287684217717']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before resize (251, 303, 303)\n",
      "after resize (135, 180, 180)\n",
      "ID00371637202296828615743 ['../dataset/train/ID00371637202296828615743']\n",
      "before resize (364, 336, 336)\n",
      "after resize (135, 180, 180)\n",
      "ID00335637202286784464927 ['../dataset/train/ID00335637202286784464927']\n",
      "before resize (30, 359, 359)\n",
      "after resize (135, 180, 180)\n",
      "ID00419637202311204720264 ['../dataset/train/ID00419637202311204720264']\n",
      "before resize (35, 420, 420)\n",
      "after resize (135, 180, 180)\n",
      "ID00134637202223873059688 ['../dataset/train/ID00134637202223873059688']\n",
      "before resize (451, 379, 379)\n",
      "after resize (135, 180, 180)\n",
      "ID00169637202238024117706 ['../dataset/train/ID00169637202238024117706']\n",
      "before resize (288, 348, 348)\n",
      "after resize (135, 180, 180)\n",
      "ID00401637202305320178010 ['../dataset/train/ID00401637202305320178010']\n",
      "before resize (30, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00307637202282126172865 ['../dataset/train/ID00307637202282126172865']\n",
      "before resize (29, 362, 362)\n",
      "after resize (135, 180, 180)\n",
      "ID00207637202252526380974 ['../dataset/train/ID00207637202252526380974']\n",
      "before resize (320, 344, 344)\n",
      "after resize (135, 180, 180)\n",
      "ID00216637202257988213445 ['../dataset/train/ID00216637202257988213445']\n",
      "before resize (21, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00102637202206574119190 ['../dataset/train/ID00102637202206574119190']\n",
      "before resize (291, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00126637202218610655908 ['../dataset/train/ID00126637202218610655908']\n",
      "before resize (21, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00019637202178323708467 ['../dataset/train/ID00019637202178323708467']\n",
      "before resize (29, 323, 323)\n",
      "after resize (135, 180, 180)\n",
      "ID00249637202266730854017 ['../dataset/train/ID00249637202266730854017']\n",
      "before resize (319, 366, 366)\n",
      "after resize (135, 180, 180)\n",
      "ID00241637202264294508775 ['../dataset/train/ID00241637202264294508775']\n",
      "before resize (253, 400, 400)\n",
      "after resize (135, 180, 180)\n",
      "ID00423637202312137826377 ['../dataset/train/ID00423637202312137826377']\n",
      "before resize (290, 355, 355)\n",
      "after resize (135, 180, 180)\n",
      "ID00381637202299644114027 ['../dataset/train/ID00381637202299644114027']\n",
      "before resize (264, 453, 453)\n",
      "after resize (135, 180, 180)\n",
      "ID00062637202188654068490 ['../dataset/train/ID00062637202188654068490']\n",
      "before resize (30, 385, 385)\n",
      "after resize (135, 180, 180)\n",
      "ID00351637202289476567312 ['../dataset/train/ID00351637202289476567312']\n",
      "before resize (29, 340, 340)\n",
      "after resize (135, 180, 180)\n",
      "ID00233637202260580149633 ['../dataset/train/ID00233637202260580149633']\n",
      "before resize (405, 394, 394)\n",
      "after resize (135, 180, 180)\n",
      "ID00392637202302319160044 ['../dataset/train/ID00392637202302319160044']\n",
      "before resize (280, 350, 350)\n",
      "after resize (135, 180, 180)\n",
      "ID00275637202271440119890 ['../dataset/train/ID00275637202271440119890']\n",
      "before resize (284, 331, 331)\n",
      "after resize (135, 180, 180)\n",
      "ID00358637202295388077032 ['../dataset/train/ID00358637202295388077032']\n",
      "before resize (312, 376, 376)\n",
      "after resize (135, 180, 180)\n",
      "ID00072637202198161894406 ['../dataset/train/ID00072637202198161894406']\n",
      "before resize (24, 363, 363)\n",
      "after resize (135, 180, 180)\n",
      "ID00323637202285211956970 ['../dataset/train/ID00323637202285211956970']\n",
      "before resize (258, 319, 319)\n",
      "after resize (135, 180, 180)\n",
      "ID00378637202298597306391 ['../dataset/train/ID00378637202298597306391']\n",
      "before resize (346, 364, 364)\n",
      "after resize (135, 180, 180)\n",
      "ID00094637202205333947361 ['../dataset/train/ID00094637202205333947361']\n",
      "before resize (30, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00279637202272164826258 ['../dataset/train/ID00279637202272164826258']\n",
      "before resize (38, 391, 391)\n",
      "after resize (135, 180, 180)\n",
      "ID00360637202295712204040 ['../dataset/train/ID00360637202295712204040']\n",
      "before resize (582, 376, 376)\n",
      "after resize (135, 180, 180)\n",
      "ID00240637202264138860065 ['../dataset/train/ID00240637202264138860065']\n",
      "before resize (41, 380, 380)\n",
      "after resize (135, 180, 180)\n",
      "ID00077637202199102000916 ['../dataset/train/ID00077637202199102000916']\n",
      "before resize (282, 340, 340)\n",
      "after resize (135, 180, 180)\n",
      "ID00115637202211874187958 ['../dataset/train/ID00115637202211874187958']\n",
      "before resize (24, 373, 373)\n",
      "after resize (135, 180, 180)\n",
      "ID00383637202300493233675 ['../dataset/train/ID00383637202300493233675']\n",
      "before resize (32, 336, 336)\n",
      "after resize (135, 180, 180)\n",
      "ID00355637202295106567614 ['../dataset/train/ID00355637202295106567614']\n",
      "before resize (37, 348, 348)\n",
      "after resize (135, 180, 180)\n",
      "ID00405637202308359492977 ['../dataset/train/ID00405637202308359492977']\n",
      "before resize (30, 352, 352)\n",
      "after resize (135, 180, 180)\n",
      "ID00015637202177877247924 ['../dataset/train/ID00015637202177877247924']\n",
      "before resize (236, 401, 401)\n",
      "after resize (135, 180, 180)\n",
      "ID00025637202179541264076 ['../dataset/train/ID00025637202179541264076']\n",
      "before resize (30, 342, 342)\n",
      "after resize (135, 180, 180)\n",
      "ID00089637202204675567570 ['../dataset/train/ID00089637202204675567570']\n",
      "before resize (36, 313, 313)\n",
      "after resize (135, 180, 180)\n",
      "ID00221637202258717315571 ['../dataset/train/ID00221637202258717315571']\n",
      "before resize (291, 376, 376)\n",
      "after resize (135, 180, 180)\n",
      "ID00197637202246865691526 ['../dataset/train/ID00197637202246865691526']\n",
      "before resize (306, 345, 345)\n",
      "after resize (135, 180, 180)\n",
      "ID00125637202218590429387 ['../dataset/train/ID00125637202218590429387']\n",
      "before resize (28, 403, 403)\n",
      "after resize (135, 180, 180)\n",
      "ID00135637202224630271439 ['../dataset/train/ID00135637202224630271439']\n",
      "before resize (337, 379, 379)\n",
      "after resize (135, 180, 180)\n",
      "ID00342637202287526592911 ['../dataset/train/ID00342637202287526592911']\n",
      "before resize (34, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00251637202267455595113 ['../dataset/train/ID00251637202267455595113']\n",
      "before resize (28, 300, 300)\n",
      "after resize (135, 180, 180)\n",
      "ID00336637202286801879145 ['../dataset/train/ID00336637202286801879145']\n",
      "before resize (33, 333, 333)\n",
      "after resize (135, 180, 180)\n",
      "ID00210637202257228694086 ['../dataset/train/ID00210637202257228694086']\n",
      "before resize (303, 356, 356)\n",
      "after resize (135, 180, 180)\n",
      "ID00128637202219474716089 ['../dataset/train/ID00128637202219474716089']\n",
      "before resize (48, 320, 320)\n",
      "after resize (135, 180, 180)\n",
      "ID00339637202287377736231 ['../dataset/train/ID00339637202287377736231']\n",
      "before resize (56, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00224637202259281193413 ['../dataset/train/ID00224637202259281193413']\n",
      "before resize (54, 382, 382)\n",
      "after resize (135, 180, 180)\n",
      "ID00242637202264759739921 ['../dataset/train/ID00242637202264759739921']\n",
      "before resize (22, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00343637202287577133798 ['../dataset/train/ID00343637202287577133798']\n",
      "before resize (49, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00130637202220059448013 ['../dataset/train/ID00130637202220059448013']\n",
      "before resize (285, 338, 338)\n",
      "after resize (135, 180, 180)\n",
      "ID00123637202217151272140 ['../dataset/train/ID00123637202217151272140']\n",
      "before resize (258, 314, 314)\n",
      "after resize (135, 180, 180)\n",
      "ID00218637202258156844710 ['../dataset/train/ID00218637202258156844710']\n",
      "before resize (320, 354, 354)\n",
      "after resize (135, 180, 180)\n",
      "ID00225637202259339837603 ['../dataset/train/ID00225637202259339837603']\n",
      "before resize (310, 333, 333)\n",
      "after resize (135, 180, 180)\n",
      "ID00294637202279614924243 ['../dataset/train/ID00294637202279614924243']\n",
      "before resize (44, 350, 350)\n",
      "after resize (135, 180, 180)\n",
      "ID00341637202287410878488 ['../dataset/train/ID00341637202287410878488']\n",
      "before resize (60, 318, 318)\n",
      "after resize (135, 180, 180)\n",
      "ID00222637202259066229764 ['../dataset/train/ID00222637202259066229764']\n",
      "before resize (58, 341, 341)\n",
      "after resize (135, 180, 180)\n",
      "ID00422637202311677017371 ['../dataset/train/ID00422637202311677017371']\n",
      "before resize (296, 340, 340)\n",
      "after resize (135, 180, 180)\n",
      "ID00105637202208831864134 ['../dataset/train/ID00105637202208831864134']\n",
      "before resize (265, 410, 410)\n",
      "after resize (135, 180, 180)\n",
      "ID00038637202182690843176 ['../dataset/train/ID00038637202182690843176']\n",
      "before resize (346, 362, 362)\n",
      "after resize (135, 180, 180)\n",
      "ID00012637202177665765362 ['../dataset/train/ID00012637202177665765362']\n",
      "before resize (343, 320, 320)\n",
      "after resize (135, 180, 180)\n",
      "ID00264637202270643353440 ['../dataset/train/ID00264637202270643353440']\n",
      "before resize (396, 400, 400)\n",
      "after resize (135, 180, 180)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID00149637202232704462834 ['../dataset/train/ID00149637202232704462834']\n",
      "before resize (306, 300, 300)\n",
      "after resize (135, 180, 180)\n",
      "ID00026637202179561894768 ['../dataset/train/ID00026637202179561894768']\n",
      "before resize (239, 374, 374)\n",
      "after resize (135, 180, 180)\n",
      "ID00060637202187965290703 ['../dataset/train/ID00060637202187965290703']\n",
      "before resize (275, 366, 366)\n",
      "after resize (135, 180, 180)\n",
      "ID00229637202260254240583 ['../dataset/train/ID00229637202260254240583']\n",
      "before resize (17, 400, 400)\n",
      "after resize (135, 180, 180)\n",
      "ID00140637202231728595149 ['../dataset/train/ID00140637202231728595149']\n",
      "before resize (813, 379, 379)\n",
      "after resize (135, 180, 180)\n",
      "ID00007637202177411956430 ['../dataset/train/ID00007637202177411956430']\n",
      "before resize (38, 334, 334)\n",
      "after resize (135, 180, 180)\n",
      "ID00020637202178344345685 ['../dataset/train/ID00020637202178344345685']\n",
      "before resize (246, 320, 320)\n",
      "after resize (135, 180, 180)\n",
      "ID00370637202296737666151 ['../dataset/train/ID00370637202296737666151']\n",
      "before resize (271, 366, 366)\n",
      "after resize (135, 180, 180)\n",
      "ID00309637202282195513787 ['../dataset/train/ID00309637202282195513787']\n",
      "before resize (704, 370, 370)\n",
      "after resize (135, 180, 180)\n",
      "ID00248637202266698862378 ['../dataset/train/ID00248637202266698862378']\n",
      "before resize (20, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00131637202220424084844 ['../dataset/train/ID00131637202220424084844']\n",
      "before resize (630, 355, 355)\n",
      "after resize (135, 180, 180)\n",
      "ID00288637202279148973731 ['../dataset/train/ID00288637202279148973731']\n",
      "before resize (26, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00368637202296470751086 ['../dataset/train/ID00368637202296470751086']\n",
      "before resize (682, 408, 408)\n",
      "after resize (135, 180, 180)\n",
      "ID00109637202210454292264 ['../dataset/train/ID00109637202210454292264']\n",
      "before resize (375, 397, 397)\n",
      "after resize (135, 180, 180)\n",
      "ID00290637202279304677843 ['../dataset/train/ID00290637202279304677843']\n",
      "before resize (300, 360, 360)\n",
      "after resize (135, 180, 180)\n",
      "ID00010637202177584971671 ['../dataset/train/ID00010637202177584971671']\n",
      "before resize (212, 335, 335)\n",
      "after resize (135, 180, 180)\n",
      "ID00183637202241995351650 ['../dataset/train/ID00183637202241995351650']\n",
      "before resize (35, 364, 364)\n",
      "after resize (135, 180, 180)\n",
      "ID00035637202182204917484 ['../dataset/train/ID00035637202182204917484']\n",
      "before resize (287, 350, 350)\n",
      "after resize (135, 180, 180)\n",
      "ID00234637202261078001846 ['../dataset/train/ID00234637202261078001846']\n"
     ]
    }
   ],
   "source": [
    "# load all dicom\n",
    "# import gdcm\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "PATH = '../dataset/output/train'\n",
    "# PATH_TEST = '../dataset/output/test'\n",
    "\n",
    "z_med_size = 135\n",
    "y_med_size = 180\n",
    "x_med_size = 180\n",
    "\n",
    "# create train data npy\n",
    "for k, v in dicom_dict.items():\n",
    "    print(k ,v)\n",
    "    if (k != 'ID00011637202177653955184' and k != 'ID00052637202186188008618'):\n",
    "        \n",
    "        scan = load_scan(v[0])\n",
    "        scan_hu = get_pixels_hu(scan)\n",
    "\n",
    "        x_size = scan[0].Columns\n",
    "        y_size = scan[0].Rows\n",
    "        slice_thinkness = scan[0].SliceThickness\n",
    "        x_pixel_spacing = scan[0].PixelSpacing[0] * (x_size / default_image_size)\n",
    "        y_pixel_spacing = scan[0].PixelSpacing[1] * (y_size / default_image_size)\n",
    "\n",
    "        # # if ratio is not 1:1 then crop\n",
    "        if (x_size != y_size):\n",
    "            scan_hu = crop(scan_hu)\n",
    "            # if cropped, reset pixel spacing with cropped image size because images with margin merely have space\n",
    "            # outter side of image so pixel spacing size shouldn't be affected by those space\n",
    "            x_pixel_spacing = scan[0].PixelSpacing[0] * (scan_hu.shape[2] / default_image_size)\n",
    "            y_pixel_spacing = scan[0].PixelSpacing[1] * (scan_hu.shape[1] / default_image_size)\n",
    "\n",
    "        # # if size is not 512 then resize\n",
    "        if (x_size != default_image_size or y_size != default_image_size):\n",
    "            scan_hu = resize(scan_hu)\n",
    "\n",
    "        # # resample test\n",
    "        resampled_scan_hu, spacing = resample(scan_hu, slice_thinkness, x_pixel_spacing, y_pixel_spacing)\n",
    "        print('before resize', resampled_scan_hu.shape)\n",
    "\n",
    "        z_ratio = z_med_size / resampled_scan_hu.shape[0]\n",
    "        y_ratio = y_med_size / resampled_scan_hu.shape[1]\n",
    "        x_ratio = x_med_size / resampled_scan_hu.shape[2]\n",
    "        resampled_scan_hu = zoom(resampled_scan_hu, (z_ratio, y_ratio, x_ratio))\n",
    "        print('after resize', resampled_scan_hu.shape)\n",
    "        \n",
    "        segmented_lungs = segment_lung_mask(resampled_scan_hu, False)\n",
    "        segmented_lungs_fill = segment_lung_mask(resampled_scan_hu, True)\n",
    "        np.save(f'{PATH}/{k}' , (segmented_lungs_fill - segmented_lungs) * -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load all dicom\n",
    "# # import gdcm\n",
    "\n",
    "# PATH = '../data/output/train'\n",
    "# PATH_TEST = '../data/output/test'\n",
    "\n",
    "\n",
    "# keys = []\n",
    "# shapes = []\n",
    "\n",
    "# # create train data npy\n",
    "# for k, v in dicom_dict.items():\n",
    "#     print(k ,v)\n",
    "#     if (k != 'ID00011637202177653955184' and k != 'ID00052637202186188008618'):\n",
    "        \n",
    "#         scan = load_scan(v[0])\n",
    "#         scan_hu = get_pixels_hu(scan)\n",
    "\n",
    "#         x_size = scan[0].Columns\n",
    "#         y_size = scan[0].Rows\n",
    "#         slice_thinkness = scan[0].SliceThickness\n",
    "#         x_pixel_spacing = scan[0].PixelSpacing[0] * (x_size / default_image_size)\n",
    "#         y_pixel_spacing = scan[0].PixelSpacing[1] * (y_size / default_image_size)\n",
    "# #         x_pixel_spacing = scan[0].PixelSpacing[0]\n",
    "# #         y_pixel_spacing = scan[0].PixelSpacing[1]\n",
    "\n",
    "#         # # if ratio is not 1:1 then crop\n",
    "#         if (x_size != y_size):\n",
    "#             scan_hu = crop(scan_hu)\n",
    "\n",
    "#         # # if size is not 512 then resize\n",
    "#         if (x_size != default_image_size or y_size != default_image_size):\n",
    "#             scan_hu = resize(scan_hu)\n",
    "\n",
    "#         # # resample test\n",
    "#         resampled_test_hu, spacing = resample(scan_hu, slice_thinkness, x_pixel_spacing, y_pixel_spacing)\n",
    "#         print(resampled_test_hu.shape)\n",
    "# #         if (resampled_test_hu.shape[1] < minimum[1]):\n",
    "# #             minimum = resampled_test_hu.shape\n",
    "# #             min_key = k\n",
    "# #         if(resampled_test_hu.shape[1] > maximum[1]):\n",
    "# #             maximum = resampled_test_hu.shape\n",
    "# #             max_key = k\n",
    "            \n",
    "# #         if (resampled_test_hu.shape[0] < z_minimum[0]):\n",
    "# #             z_minimum = resampled_test_hu.shape\n",
    "# #             z_min_key = k\n",
    "# #         if(resampled_test_hu.shape[0] > z_maximum[0]):\n",
    "# #             z_maximum = resampled_test_hu.shape\n",
    "# #             z_max_key = k\n",
    "#         keys.append(k)\n",
    "#         shapes.append(resampled_test_hu.shape)\n",
    "\n",
    "\n",
    "# # print('minimum', min_key, minimum)\n",
    "# # print('maximim', max_key, maximum)\n",
    "# # print('z_minimum', z_min_key, z_minimum)\n",
    "# # print('z_maximim', z_max_key, z_maximum)\n",
    "# #     segmented_lungs = segment_lung_mask(resample_scan_hu, False)\n",
    "# #     segmented_lungs_fill = segment_lung_mask(resample_scan_hu, True)\n",
    "# #     np.save(f'{PATH}/{k}' , segmented_lungs_fill - segmented_lungs)\n",
    "    \n",
    "# # create test data npy\n",
    "# # for k, v in dicom_dict_test.items():\n",
    "# #     print(k ,v)\n",
    "# #     scan = load_scan(v[0])\n",
    "# #     scan_hu = get_pixels_hu(scan)\n",
    "# #     resample_scan_hu, spacing = resample(scan_hu, scan)\n",
    "# #     segmented_lungs = segment_lung_mask(resample_scan_hu, False)\n",
    "# #     segmented_lungs_fill = segment_lung_mask(resample_scan_hu, True)\n",
    "# #     np.save(f'{PATH_TEST}/{k}' , segmented_lungs_fill - segmented_lungs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# x_median = np.median([x[1] for x in shapes])\n",
    "# z_median = np.median([x[0] for x in shapes])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
